{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 posts in training set: \n",
      " [['the', 'cdc', 'currently', 'reports', '99031', 'deaths', '.', 'in', 'general', 'the', 'discrepancies', 'in', 'death', 'counts', 'between', 'different', 'sources', 'are', 'small', 'and', 'explicable', '.', 'the', 'death', 'toll', 'stands', 'at', 'roughly', '100000', 'people', 'today', '.'], ['states', 'reported', '1121', 'deaths', 'a', 'small', 'rise', 'from', 'last', 'tuesday', '.', 'southern', 'states', 'reported', '640', 'of', 'those', 'deaths', '.', 'https://t', '.', 'co/yasgrtt4ux'], ['politically', 'correct', 'woman', '(almost)', 'uses', 'pandemic', 'as', 'excuse', 'not', 'to', 'reuse', 'plastic', 'bag', 'https://t', '.', 'co/thf8gunfpe', '#', 'coronavirus', '#', 'nashville'], ['#', 'indiafightscorona:', 'we', 'have', '1524', '#', 'covid', 'testing', 'laboratories', 'in', 'india', 'and', 'as', 'on', '25th', 'august', '2020', '36827520', 'tests', 'have', 'been', 'done', ':', '@', 'profbhargava', 'dg', '@', 'icmrdelhi', '#', 'staysafe', '#', 'indiawillwin', 'https://t', '.', 'co/yh3zxknnhz'], ['populous', 'states', 'can', 'generate', 'large', 'case', 'counts', 'but', 'if', 'you', 'look', 'at', 'the', 'new', 'cases', 'per', 'million', 'today', '9', 'smaller', 'states', 'are', 'showing', 'more', 'cases', 'per', 'million', 'than', 'california', 'or', 'texas:', 'al', 'ar', 'id', 'ks', 'ky', 'la', 'ms', 'nv', 'and', 'sc', '.', 'https://t', '.', 'co/1pyw6cwras'], ['covid', 'act', 'now', 'found', '\"on', 'average', 'each', 'person', 'in', 'illinois', 'with', 'covid-19', 'is', 'infecting', '1', '.', '11', 'other', 'people', '.', 'data', 'shows', 'that', 'the', 'infection', 'growth', 'rate', 'has', 'declined', 'over', 'time', 'this', 'factors', 'in', 'the', 'stay-at-home', 'order', 'and', 'other', 'restrictions', 'put', 'in', 'place', '.', '\"', 'https://t', '.', 'co/hhigdd24fe'], ['if', 'you', 'tested', 'positive', 'for', '#', 'covid19', 'and', 'have', 'no', 'symptoms', 'stay', 'home', 'and', 'away', 'from', 'other', 'people', '.', 'learn', 'more', 'about', 'cdcâ€™s', 'recommendations', 'about', 'when', 'you', 'can', 'be', 'around', 'others', 'after', 'covid-19', 'infection:', 'https://t', '.', 'co/z5kkxpqkyb', '.', 'https://t', '.', 'co/9pamy0rxaf'], ['obama', 'calls', 'trumpâ€™s', 'coronavirus', 'response', 'a', 'chaotic', 'disaster', 'https://t', '.', 'co/dedqzehasb'], ['?', '?', '?', 'clearly', ',', 'the', 'obama', 'administration', 'did', 'not', 'leave', 'any', 'kind', 'of', 'game', 'plan', 'for', 'something', 'like', 'this', '.', '?', '?', 'ï¿½'], ['retractionâ€”hydroxychloroquine', 'or', 'chloroquine', 'with', 'or', 'without', 'a', 'macrolide', 'for', 'treatment', 'of', 'covid-19:', 'a', 'multinational', 'registry', 'analysis', '-', 'the', 'lancet', 'https://t', '.', 'co/l5v2x6g9or']]\n",
      "- Number of datapoints in training set:  5604\n",
      "- Average tweet length in train: 28.586366880799428\n",
      "- Average num charachters in tweet in train:  138.05442541042112\n",
      "- Num tweets removed because they were longer than 280 charachters:  816\n",
      "- Number of unique words in train:  19092\n",
      "- 10 most common words in train:  [('.', 6.79), ('the', 3.37), ('#', 2.27), ('of', 2.17), ('https://t', 2.11), ('to', 2.0), ('in', 1.87), ('a', 1.52), ('and', 1.37), ('is', 1.03)]\n",
      "- Total words in train: 160198\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np \n",
    "\n",
    "tweets = []\n",
    "path = 'data/Constraint_Train.csv'\n",
    "num_long_posts = 0\n",
    "\n",
    "with open(path, newline='', encoding='utf-8') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "    spamreader.__next__()  # Skip header row\n",
    "    for row in spamreader:\n",
    "        if len(row[1]) > 280:\n",
    "            num_long_posts += 1\n",
    "        else: \n",
    "            row[1] = row[1].replace(',', ' , ')\\\n",
    "                .replace(\"'\", \" ' \")\\\n",
    "                .replace('.', ' . ')\\\n",
    "                .replace('!', ' ! ')\\\n",
    "                .replace('?', ' ? ')\\\n",
    "                .replace(';', ' ; ')\\\n",
    "                .replace('@', ' @ ')\\\n",
    "                .replace('#', ' # ')\n",
    "            words = row[1].split()\n",
    "            t = 0 if row[2] == 'real' else 1\n",
    "            sentence = [word.lower() for word in words]\n",
    "            tweets.append(sentence)\n",
    "\n",
    "vocab = set([w for s in tweets for w in s])\n",
    "\n",
    "train = tweets[:]\n",
    "\n",
    "print(\"First 10 posts in training set: \\n\", train[:10])\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(\"- Number of datapoints in training set: \", len(tweets))\n",
    "lengths = [len(tweet) for tweet in train]\n",
    "print(\"- Average tweet length in train:\", np.mean(lengths))\n",
    "chars = []\n",
    "for tweet in train:\n",
    "    length = 0\n",
    "    for word in tweet:\n",
    "        length += len(word)\n",
    "    chars.append(length) \n",
    "print(\"- Average num charachters in tweet in train: \", np.mean(chars))\n",
    "print(\"- Num tweets removed because they were longer than 280 charachters: \", num_long_posts)\n",
    "words = [word for tweet in train for word in tweet]\n",
    "cnt = Counter(words)\n",
    "print(\"- Number of unique words in train: \", len(cnt.keys()))\n",
    "print(\"- 10 most common words in train: \", [(i, round(cnt[i] / len(words) * 100.0, 2)) for i, ntount in cnt.most_common(10)])\n",
    "tot = np.sum(list(cnt.values()))\n",
    "print(\"- Total words in train:\", tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 posts in validation set: \n",
      " [['chinese', 'converting', 'to', 'islam', 'after', 'realising', 'that', 'no', 'muslim', 'was', 'affected', 'by', '#', 'coronavirus', '#', 'covd19', 'in', 'the', 'country'], ['11', 'out', 'of', '13', 'people', '(from', 'the', 'diamond', 'princess', 'cruise', 'ship)', 'who', 'had', 'intially', 'tested', 'negative', 'in', 'tests', 'in', 'japan', 'were', 'later', 'confirmed', 'to', 'be', 'positive', 'in', 'the', 'united', 'states', '.'], ['covid-19', 'is', 'caused', 'by', 'a', 'bacterium', ',', 'not', 'virus', 'and', 'can', 'be', 'treated', 'with', 'aspirin'], ['mike', 'pence', 'in', 'rnc', 'speech', 'praises', 'donald', 'trumpâ€™s', 'covid-19', 'â€œseamlessâ€', 'partnership', 'with', 'governors', 'and', 'leaves', 'out', 'the', 'president', \"'\", 's', 'state', 'feuds:', 'https://t', '.', 'co/qj6hsewtgb', '#', 'rnc2020', 'https://t', '.', 'co/ofoerzdfyy'], ['6/10', 'sky', \"'\", 's', '@', 'edconwaysky', 'explains', 'the', 'latest', '#', 'covid19', 'data', 'and', 'government', 'announcement', '.', 'get', 'more', 'on', 'the', '#', 'coronavirus', 'data', 'hereðŸ‘‡', 'https://t', '.', 'co/jvgzlsbfjh', 'https://t', '.', 'co/pygskxesbg'], ['no', 'one', 'can', 'leave', 'managed', 'isolation', 'for', 'any', 'reason', 'without', 'returning', 'a', 'negative', 'test', '.', 'if', 'they', 'refuse', 'a', 'test', 'they', 'can', 'then', 'be', 'held', 'for', 'a', 'period', 'of', 'up', 'to', '28', 'days', '.', '\\u2063', '\\u2063', 'on', 'june', 'the', '16th', 'exemptions', 'on', 'compassionate', 'grounds', 'have', 'been', 'suspended', '.', '\\u2063', '\\u2063'], ['#', 'indiafightscorona', 'india', 'has', 'one', 'of', 'the', 'lowest', '#', 'covid19', 'mortality', 'globally', 'with', 'less', 'than', '2%', 'case', 'fatality', 'rate', '.', 'as', 'a', 'result', 'of', 'supervised', 'home', 'isolation', '&amp', ';', 'effective', 'clinical', 'treatment', 'many', 'states/uts', 'have', 'cfr', 'lower', 'than', 'the', 'national', 'average', '.', 'https://t', '.', 'co/qlik8ypp7e'], ['rt', '@', 'who:', '#', 'covid19', 'transmission', 'occurs', 'primarily', 'through', 'direct', 'indirect', 'or', 'close', 'contact', 'with', 'infected', 'people', 'through', 'their', 'saliva', 'and', 'resâ€¦'], ['news', 'and', 'media', 'outlet', 'abp', 'majha', 'on', 'the', 'basis', 'of', 'an', 'internal', 'memo', 'of', 'south', 'central', 'railway', 'reported', 'that', 'a', 'special', 'train', 'has', 'been', 'announced', 'to', 'take', 'the', 'stranded', 'migrant', 'workers', 'home', '.'], ['?', '?', '?', 'church', 'services', 'can', '?', '?', '?', 't', 'resume', 'until', 'we', '?', '?', '?', 're', 'all', 'vaccinated', ',', 'says', 'bill', 'gates', '.', '?', '?', 'ï¿½']]\n",
      "- Number of datapoints in validation set:  1873\n",
      "- Average tweet length in val: 28.60758142018153\n",
      "- Average num charachters in tweet in val:  137.95835557928456\n",
      "- Num tweets removed because they were longer than 280 charachters:  267\n",
      "- Number of unique words in val:  9128\n",
      "- 10 most common words in val:  [('.', 6.82), ('the', 3.39), ('#', 2.18), ('of', 2.17), ('https://t', 2.08), ('to', 1.96), ('in', 1.94), ('a', 1.51), ('and', 1.3), ('is', 1.08)]\n",
      "- Total words in val: 53582\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np \n",
    "\n",
    "tweets = []\n",
    "path = 'data/Constraint_Val.csv'\n",
    "num_long_posts = 0\n",
    "\n",
    "with open(path, newline='', encoding='utf-8') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "    spamreader.__next__()  # Skip header row\n",
    "    for row in spamreader:\n",
    "        if len(row[1]) > 280:\n",
    "            num_long_posts += 1\n",
    "        else: \n",
    "            row[1] = row[1].replace(',', ' , ')\\\n",
    "                .replace(\"'\", \" ' \")\\\n",
    "                .replace('.', ' . ')\\\n",
    "                .replace('!', ' ! ')\\\n",
    "                .replace('?', ' ? ')\\\n",
    "                .replace(';', ' ; ')\\\n",
    "                .replace('@', ' @ ')\\\n",
    "                .replace('#', ' # ')\n",
    "            words = row[1].split()\n",
    "            t = 0 if row[2] == 'real' else 1\n",
    "            sentence = [word.lower() for word in words]\n",
    "            tweets.append(sentence)\n",
    "\n",
    "vocab = set([w for s in tweets for w in s])\n",
    "\n",
    "val = tweets[:]\n",
    "\n",
    "print(\"First 10 posts in validation set: \\n\", val[:10])\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(\"- Number of datapoints in validation set: \", len(tweets))\n",
    "lengths = [len(tweet) for tweet in val]\n",
    "print(\"- Average tweet length in val:\", np.mean(lengths))\n",
    "chars = []\n",
    "for tweet in val:\n",
    "    length = 0\n",
    "    for word in tweet:\n",
    "        length += len(word)\n",
    "    chars.append(length) \n",
    "print(\"- Average num charachters in tweet in val: \", np.mean(chars))\n",
    "print(\"- Num tweets removed because they were longer than 280 charachters: \", num_long_posts)\n",
    "words = [word for tweet in val for word in tweet]\n",
    "cnt = Counter(words)\n",
    "print(\"- Number of unique words in val: \", len(cnt.keys()))\n",
    "print(\"- 10 most common words in val: \", [(i, round(cnt[i] / len(words) * 100.0, 2)) for i, ntount in cnt.most_common(10)])\n",
    "tot = np.sum(list(cnt.values()))\n",
    "print(\"- Total words in val:\", tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 posts in test set: \n",
      " [['our', 'daily', 'update', 'is', 'published', '.', 'states', 'reported', '734k', 'tests', '39k', 'new', 'cases', 'and', '532', 'deaths', '.', 'current', 'hospitalizations', 'fell', 'below', '30k', 'for', 'the', 'first', 'time', 'since', 'june', '22', '.', 'https://t', '.', 'co/wzsyme0sht'], ['alfalfa', 'is', 'the', 'only', 'cure', 'for', 'covid-19', '.'], ['president', 'trump', 'asked', 'what', 'he', 'would', 'do', 'if', 'he', 'were', 'to', 'catch', 'the', 'coronavirus', 'https://t', '.', 'co/3mewhusrzi', '#', 'donaldtrump', '#', 'coronavirus'], ['states', 'reported', '630', 'deaths', '.', 'we', 'are', 'still', 'seeing', 'a', 'solid', 'national', 'decline', '.', 'death', 'reporting', 'lags', 'approximately', '28', 'days', 'from', 'symptom', 'onset', 'according', 'to', 'cdc', 'models', 'that', 'consider', 'lags', 'in', 'symptoms', 'time', 'in', 'hospital', 'and', 'the', 'death', 'reporting', 'process', '.', 'https://t', '.', 'co/lbmcot3h9a'], ['this', 'is', 'the', 'sixth', 'time', 'a', 'global', 'health', 'emergency', 'has', 'been', 'declared', 'under', 'the', 'international', 'health', 'regulations', 'but', 'it', 'is', 'easily', 'the', 'most', 'severe-', '@', 'drtedros', 'https://t', '.', 'co/jvkc0ptett'], ['low', '#', 'vitamind', 'was', 'an', 'independent', 'predictor', 'of', 'worse', 'prognosis', 'in', 'patients', 'with', 'covid-19', '.', 'https://t', '.', 'co/cgd6kphn31', 'https://t', '.', 'co/chtni8k4jd'], ['a', 'common', 'question:', 'why', 'are', 'the', 'cumulative', 'outcome', 'numbers', 'smaller', 'than', 'the', 'current', 'outcome', 'numbers', '?', 'a:', 'most', 'states', 'report', 'current', 'but', 'a', 'few', 'states', 'report', 'cumulative', '.', 'they', 'are', 'apples', 'and', 'oranges', 'and', 'we', 'don', \"'\", 't', 'feel', 'comfortable', 'filling', 'in', 'state', 'cumulative', 'boxes', 'with', 'current', '#', 's', '.'], ['the', 'government', 'should', 'consider', 'bringing', 'in', 'any', 'new', 'national', 'lockdown', 'rules', 'over', 'christmas', 'rather', 'than', 'now', 'says', 'an', 'oxford', 'university', 'professor', 'https://t', '.', 'co/pdols6cqon'], ['two', 'interesting', 'correlations:', '1)', 'children', 'tend', 'to', 'weather', 'covid-19', 'pretty', 'well', ';', 'they', 'also', 'get', 'a', 'ton', 'of', 'vitamin', 'd', '.', '2)', 'black', 'people', 'are', 'getting', 'slammed', 'by', 'covid-19', ';', 'black', 'people', 'also', 'have', 'much', 'higher', 'instances', 'of', 'vitamin', 'd', 'deficiency', '(76%', 'vs', '40%', 'in', 'the', 'general', 'population)', '.'], ['a', 'photo', 'shows', 'a', '19-year-old', 'vaccine', 'for', 'canine', 'coronavirus', 'that', 'could', 'be', 'used', 'to', 'prevent', 'the', 'new', 'coronavirus', 'causing', 'covid-19', '.']]\n",
      "- Number of datapoints in test set:  1848\n",
      "- Average tweet length in test: 28.33874458874459\n",
      "- Average num charachters in tweet in test:  136.79978354978354\n",
      "- Num tweets removed because they were longer than 280 charachters:  292\n",
      "- Number of unique words in test:  8990\n",
      "- 10 most common words in test:  [('.', 6.72), ('the', 3.4), ('#', 2.42), ('of', 2.13), ('https://t', 2.11), ('to', 1.92), ('in', 1.85), ('a', 1.59), ('and', 1.3), ('?', 1.03)]\n",
      "- Total words in test: 52370\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np \n",
    "\n",
    "tweets = []\n",
    "path = 'data/english_test_with_labels.csv'\n",
    "num_long_posts = 0\n",
    "\n",
    "with open(path, newline='', encoding='utf-8') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "    spamreader.__next__()  # Skip header row\n",
    "    for row in spamreader:\n",
    "        if len(row[1]) > 280:\n",
    "            num_long_posts += 1\n",
    "        else: \n",
    "            row[1] = row[1].replace(',', ' , ')\\\n",
    "                .replace(\"'\", \" ' \")\\\n",
    "                .replace('.', ' . ')\\\n",
    "                .replace('!', ' ! ')\\\n",
    "                .replace('?', ' ? ')\\\n",
    "                .replace(';', ' ; ')\\\n",
    "                .replace('@', ' @ ')\\\n",
    "                .replace('#', ' # ')\n",
    "            words = row[1].split()\n",
    "            t = 0 if row[2] == 'real' else 1\n",
    "            sentence = [word.lower() for word in words]\n",
    "            tweets.append(sentence)\n",
    "\n",
    "vocab = set([w for s in tweets for w in s])\n",
    "\n",
    "test = tweets[:]\n",
    "\n",
    "print(\"First 10 posts in test set: \\n\", test[:10])\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(\"- Number of datapoints in test set: \", len(tweets))\n",
    "lengths = [len(tweet) for tweet in test]\n",
    "print(\"- Average tweet length in test:\", np.mean(lengths))\n",
    "chars = []\n",
    "for tweet in test:\n",
    "    length = 0\n",
    "    for word in tweet:\n",
    "        length += len(word)\n",
    "    chars.append(length) \n",
    "print(\"- Average num charachters in tweet in test: \", np.mean(chars))\n",
    "print(\"- Num tweets removed because they were longer than 280 charachters: \", num_long_posts)\n",
    "words = [word for tweet in test for word in tweet]\n",
    "cnt = Counter(words)\n",
    "print(\"- Number of unique words in test: \", len(cnt.keys()))\n",
    "print(\"- 10 most common words in test: \", [(i, round(cnt[i] / len(words) * 100.0, 2)) for i, ntount in cnt.most_common(10)])\n",
    "tot = np.sum(list(cnt.values()))\n",
    "print(\"- Total words in test:\", tot)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
